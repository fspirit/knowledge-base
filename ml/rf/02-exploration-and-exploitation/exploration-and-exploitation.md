# Exploration and Exploitation

## Resources

- [Reinforcement Learning 2: Exploration and Exploitation](https://youtu.be/eM6IBYVqXEA) 
- [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/bookdraft2018jan1.pdf) - Chapter 2: Multi-armed bandits

## Summary

**Exploitation**: Maximize performance based on current knowledge

**Exploration**: Increase knowledge

The best long-term strategy may involve short-term sacrifices.

We want to gather enough information to make the best overall decisitons.

### Multi-armed bandit

- We have a discrete set of actions A
- At each step t agent selects action At
- Environmnet generates a **random** reward Rt
- The distribution p(r | a) is fixed but unknown
- The goal is to mamimize the cumulative reward `sum(Ri)`.
(We sum over a lifetime of an agent). This also menas we have to optimally balance learning and doing. No separate training and testing times.
- Usually this is called *a game against nature*

Action value is expected reward, when we take a

```
q(a) = E[Rt | At = a]
```

A simple estimate for above expectation is average of sampled rewards

```
Qt(a) = sum(rewards for a) / number_of_times_we_took_action_a
```

It can be calculated step by step with update rule for selected action
(see the derivation in Sutton, its trival)

```
Qt(At) = Qt-1(At) + alpha * (Rt - Qt-1(At)),
where alpha = 1 / number_of_times_we_selected_a
```

If alpha is `1 / n` the estimate Qt will converge to q(a) eventually,
because of the Law Of Large Numbers.
Not true for constant alphas or alphas which change from step to step (to give latest rewards more weight).

For non-selected actions we just do Qt = Qt-1.

### True optimal value

```
v* = max_a q(a) = max_a E[Rt|At = a]
```

The value for optimal action.

### Regret

```
regret = v* - q(a)
```

If a is optimal actions regret is 0. Otherwise its greater than 0.

The agent can not observe regret as it does not know v*. But it is a useful tool to analyze algorithms.

### How to balance expl Vs exploitation

Minimize the total regret. Which is the same as maximizing the total reward.

```
Lt = sum(v* - q(ai))
```
or, if we denote `delta_a = v* - q(a)`, for each possible action, than

```
Lt = sum(Nt(a) * delta_a), where Nt(a) - number of times action a was selected
```

There is a theorem that states, that no algorithm for this task (min total regret) can grow slower better than `log t`. That means, the best possible alrorithm will be `O(log t)`

Of course we do not know optimal action values, when acting, so these metrics only serve to compare different algorithms.

### Model-free value estimatiom based algorithms

#### Greedy

Just select the action, that has rhe best estimate Qt at the moment.

It can stuck in subopminal actions forever due to unlucky tries of optimal actions (as the Reward for each action if random).

#### Epsilon-greedy

Take greedy (best estimation at the momemt) action with probability (`1 - eps`)
Take random action with prob `eps`

Better than simple greedy in the long run.

### Upper Confidence Bounds (UCB)

The idea:

We need to take into consideration not only the current estimate Qt but also the uncertainily of that estimate, which is large initially for all actions, but that goes down as we choose action many times.

So for each action we introduce a bonus `Ut` which we add to the estimate, such that

```
q(a) <= Qt + Ut
```
with sufficiently large probability.

So we select action with
```
At = argmax_a (Qt(a) + Ut(a))
```
We want 
- Small Nt(a) -> large Ut, if we select a seldom, we want the uncertainity about it to raise
- Large Nt(a) -> low Ut

Next we can derive the value of Ut from Hoeffding inequality, assuming we estomate Qt as average, the Hoeffding inequality will be

```
p(q(a) >= Qt + Ut) <= exp(-2 * Nt * Ut^2) 
```
which means the large we select Ut or the more often we select action (Nt is large) the smaller is prob that true value of action is greater than `Qt + Ut`, which means that the prob `q(a) < Qt + Ut` is big.

Than we fix `p = exp(-2 * Nt * Ut^2)`. Convert inequality to equality and solve for Ut. We get

```
Ut = sqrt( - ln(p) / 2 * Nt)
```

From this we can get Ut if we have Nt and we want to set p. (for instance p = 0.05). We will get Ut that will make `p(q(a) >= Qt + Ut) <= 0.05` true.

Next idea:

Not only we want to select Ut that gives us high prob of q(a) to be <=  Qt + Ut, but also we want to reduce p over time. So we make `p = 1 / t`. Which gives us 

```
Ut(a) = sqrt(log t / 2 * Nt(a)), as -log p = log t
```

Which leads to the

```
At = argmax_a (Qt(a) + c* sqrt(log t / Nt(a)))
```

There is a Theorem, which says that this algo gives `Lt = O(log t)`.

### Model-based, distribution params estimation bayesian algos

Idea:

1 Try to determina params theta of `Rt | theta, a` distribution, which generates rewards, conditioned on actions.
2 Insert some prior knowledge about `theta | a` distribitions. Like, we know that for most actions `Rt | a` distrubution will be Bernoulli and have p = 0.5.

Than we step by step update our distributions over params theta

```
p_next( theta |a) = p (Rt | theta, a) * p_prev (theta | a)
```

If our we select convinient priors we will get convinient posteriors. If `Rt |a` is Bernolli, we have just one param `theta`. At first step we set `theta | a` distribution to uniform on [0, 1]. After we do one update we get `theat | a` to become Beta distribution and continue with it.

Having these distributions we can:

#### Probability matering

Just select action, which has largest mean of its `Rt | a` distribution.

#### Thompson sampling

At each step sample one value for each `Rt | a` distribution and select action which gives largest sample.

**Both of these appraches reach optimal performance. But usually calculating posteriors is not easy, unless you have convinient conjugate priors**

### Information state approach

### Direct policy estimation approach

Idea:

Parametrize policy and learn its parameters. 

Example. Policy takes from of softmax

```
PI(a) = exp(Ht(a)) / sum(exp(Ht(b))), where Ht(a) is some preference for action a
```

Here we have no direct notion of value, but true values of course influence these preference values.

Next idea:

Update policy params using gradient ascent.

```
theta = theta + alpha * grad_theta(E[Rt | theta])
```

But, we dont have E[Rt | theta]. So we do a log-likelihood (REINFORCE) trick. (It based on grad(f(a)) / f(a) = grad(log(f(a))))

And we come to
```
grad of E[Rt | theta] = E[Rt * grad_theta of (log PI)], where PI is policy function
And general learning rule:
theta = theta + alpha * Rt * grad_theta of (log PI)
```

After that we go back to softmax and construct update rules for Ht

```
Ht+1(a) = Ht(a) + alpha * Rt*(1 - PI(a)) if action a is selected for this step, and
Ht+1 = Ht(a) - alpha * Rt * PI(a) for other actions
```
















